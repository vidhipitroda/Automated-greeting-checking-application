{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/vidhi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import preprocessing\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import matplotlib\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.utils import resample\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.datasets import load_files\n",
    "nltk.download('stopwords')\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the dataset and loading it\n",
    "csv_file = pd.read_csv('/Users/vidhi/Desktop/Assesments/rsics_dataset/tagged_selections_by_sentence.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/vidhi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the labels and selected sentences are taken from the file and stored in dataframe\n",
    "selected = csv_file['Selected']\n",
    "labels = csv_file['Greeting']\n",
    "df = pd.DataFrame(selected)\n",
    "labels = pd.DataFrame(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the both columns\n",
    "dataf = csv_file[['Selected', 'Greeting']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 5957, 1: 802})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#printing the labels total \n",
    "Counter(csv_file['Greeting'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate majority and minority classes as 1 labels are less as compared to 0\n",
    "data_majority = dataf[dataf['Greeting'] == 0]\n",
    "data_minority = dataf[dataf['Greeting'] == 1]\n",
    "bias = data_minority.shape[0]/data_majority.shape[0]\n",
    "# lets split train/test data first then \n",
    "train = pd.concat([data_majority.sample(frac=0.8,random_state=200),\n",
    "         data_minority.sample(frac=0.8,random_state=200)])\n",
    "test = pd.concat([data_majority.drop(data_majority.sample(frac=0.8,random_state=200).index),\n",
    "        data_minority.drop(data_minority.sample(frac=0.8,random_state=200).index)])\n",
    "\n",
    "train = shuffle(train)\n",
    "test = shuffle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After upsampling\n",
      "0    4766\n",
      "1    2500\n",
      "Name: Greeting, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#separating the label from training data and perform upsamping \n",
    "\n",
    "data_majority = train[train['Greeting'] == 0]\n",
    "data_minority = train[train['Greeting'] == 1]\n",
    "# Upsample minority class\n",
    "data_minority_upsampled = resample(data_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples= 2500,    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "data_upsampled = pd.concat([data_majority, data_minority_upsampled])\n",
    "\n",
    "print(\"After upsampling\\n\",data_upsampled.Greeting.value_counts(),sep = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing preprocessing \n",
    "\n",
    "def preprocessing(text):\n",
    "    stemmer = WordNetLemmatizer()\n",
    "    document = re.sub(r'\\W', ' ', str(text))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the upsampled training data \n",
    "training_data = np.array(data_upsampled['Selected'])\n",
    "testing_data = np.array(test['Selected'])\n",
    "\n",
    "y_train = np.array(data_upsampled['Greeting'])\n",
    "y_test = np.array(test['Greeting'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1351,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing the training and test data separately\n",
    "X_train = []\n",
    "X_test = []\n",
    "\n",
    "for i in range(len(training_data)):\n",
    "    X_train.append(preprocessing(training_data[i]))\n",
    "    \n",
    "for i in range(len(testing_data)):\n",
    "    X_test.append(preprocessing(testing_data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing and making a vocab\n",
    "corpus = np.array(X_train)\n",
    "vectorizer = CountVectorizer(decode_error=\"replace\")\n",
    "vec_train = vectorizer.fit_transform(corpus).toarray()\n",
    "#Save vectorizer.vocabulary_\n",
    "pickle.dump(vectorizer.vocabulary_,open(\"feature.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9606385906963941"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the data in multinomial naive bayes\n",
    "#a specialized version of naive bayes designed to handle text documents using word counts\n",
    "\n",
    "nb = MultinomialNB()\n",
    "\n",
    "nb.fit(vec_train, y_train)\n",
    "\n",
    "nb.score(vec_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(nb,open(\"classifier.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the vocab later from the saved model and predicting test data\n",
    "transformer = TfidfTransformer()\n",
    "loaded_vec = CountVectorizer(decode_error=\"replace\",vocabulary=pickle.load(open(\"feature.pkl\", \"rb\")))\n",
    "Test_data = transformer.fit_transform(loaded_vec.fit_transform(X_test)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting the test data\n",
    "y_pred = nb.predict(Test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96      1191\n",
      "           1       0.71      0.71      0.71       160\n",
      "\n",
      "    accuracy                           0.93      1351\n",
      "   macro avg       0.84      0.83      0.83      1351\n",
      "weighted avg       0.93      0.93      0.93      1351\n",
      "\n",
      "0.9311621021465581\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "#print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting a new data\n",
    "q1 = preprocessing('Hello, Both got canceled even though they were confirmed about three times. Had to take another airline back skipped last flight and took the train back instead. Had paid for the overseas flight extra for economy plus seats and of course didnt get them since the flight got canceled and we flew with another carrier. Thanks, Undine')\n",
    "list1 = []\n",
    "list1.append(q1)\n",
    "#Load it later\n",
    "transformer = TfidfTransformer()\n",
    "loaded_vec = CountVectorizer(decode_error=\"replace\",vocabulary=pickle.load(open(\"feature.pkl\", \"rb\")))\n",
    "tfidf = transformer.fit_transform(loaded_vec.fit_transform(list1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.predict(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
